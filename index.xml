<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title/><link>https://wangmerlyn.github.io/</link><description>Recent content on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Jul 2022 16:33:13 +0800</lastBuildDate><atom:link href="https://wangmerlyn.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Test</title><link>https://wangmerlyn.github.io/posts/test/</link><pubDate>Wed, 27 Jul 2022 16:33:13 +0800</pubDate><guid>https://wangmerlyn.github.io/posts/test/</guid><description>&amp;rsquo;''
&amp;rsquo;''</description></item><item><title>Good Comment</title><link>https://wangmerlyn.github.io/posts/good-comment/</link><pubDate>Wed, 27 Jul 2022 16:19:26 +0800</pubDate><guid>https://wangmerlyn.github.io/posts/good-comment/</guid><description>Talk? good?</description></item><item><title>APF review</title><link>https://wangmerlyn.github.io/posts/my-first/</link><pubDate>Tue, 26 Jul 2022 16:09:39 +0800</pubDate><guid>https://wangmerlyn.github.io/posts/my-first/</guid><description>&amp;ldquo;Communication-Efficient Federated Learning with Adaptive Parameter Freezing&amp;rdquo; review Back Ground There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.
Way to reduce the communication amount for distributed model training quantizing the update into fewer bits sparsify local updates The Challenges of boosting federal learning efficiency by skipping out of stablized parameters How to identify stabilized parameters effectively How to ensure that the model will converge to the optimal state The parameter changing feature By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{*} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^* $.</description></item></channel></rss>