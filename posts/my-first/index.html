<!doctype html><html class="not-ready text-sm lg:text-base" lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>APF review -</title><meta name=theme-color><meta name=description content="&ldquo;Communication-Efficient Federated Learning with Adaptive Parameter Freezing&rdquo; review Back Ground There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.
Way to reduce the communication amount for distributed model training quantizing the update into fewer bits sparsify local updates The Challenges of boosting federal learning efficiency by skipping out of stablized parameters How to identify stabilized parameters effectively How to ensure that the model will converge to the optimal state The parameter changing feature By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{*} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^* $."><meta name=author content="Wangmerlyn"><link rel="preload stylesheet" as=style href=https://wangmerlyn.github.io/main.min.css><link rel=preload as=image href=https://wangmerlyn.github.io/theme.%7B%7B%20if%20site.Params.monoDarkIcon%20%7D%7Dsvg%7B%7B%20else%20%7D%7Dpng%7B%7B%20end%20%7D%7D><link rel=preload as=image href="https://www.gravatar.com/avatar/4cb92498631c99b1dca3aaf0b8488462?s=160&d=identicon"><link rel=preload as=image href=https://wangmerlyn.github.io/github.svg><link rel=preload as=image href=https://wangmerlyn.github.io/rss.svg><link rel=icon href=https://wangmerlyn.github.io/favicon.ico><link rel=apple-touch-icon href=https://wangmerlyn.github.io/apple-touch-icon.png><meta name=generator content="Hugo 0.101.0"><meta property="og:title" content="APF review"><meta property="og:description" content="&ldquo;Communication-Efficient Federated Learning with Adaptive Parameter Freezing&rdquo; review Back Ground There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.
Way to reduce the communication amount for distributed model training quantizing the update into fewer bits sparsify local updates The Challenges of boosting federal learning efficiency by skipping out of stablized parameters How to identify stabilized parameters effectively How to ensure that the model will converge to the optimal state The parameter changing feature By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{*} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^* $."><meta property="og:type" content="article"><meta property="og:url" content="https://wangmerlyn.github.io/posts/my-first/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-26T16:09:39+08:00"><meta property="article:modified_time" content="2022-07-26T16:09:39+08:00"><meta itemprop=name content="APF review"><meta itemprop=description content="&ldquo;Communication-Efficient Federated Learning with Adaptive Parameter Freezing&rdquo; review Back Ground There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.
Way to reduce the communication amount for distributed model training quantizing the update into fewer bits sparsify local updates The Challenges of boosting federal learning efficiency by skipping out of stablized parameters How to identify stabilized parameters effectively How to ensure that the model will converge to the optimal state The parameter changing feature By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{*} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^* $."><meta itemprop=datePublished content="2022-07-26T16:09:39+08:00"><meta itemprop=dateModified content="2022-07-26T16:09:39+08:00"><meta itemprop=wordCount content="467"><meta itemprop=keywords content="distributed learning,AI system,"><meta name=twitter:card content="summary"><meta name=twitter:title content="APF review"><meta name=twitter:description content="&ldquo;Communication-Efficient Federated Learning with Adaptive Parameter Freezing&rdquo; review Back Ground There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.
Way to reduce the communication amount for distributed model training quantizing the update into fewer bits sparsify local updates The Challenges of boosting federal learning efficiency by skipping out of stablized parameters How to identify stabilized parameters effectively How to ensure that the model will converge to the optimal state The parameter changing feature By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{*} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^* $."></head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[5rem] max-w-3xl px-8 lg:justify-center"><div class="relative z-50 mr-auto flex items-center"><a class="-translate-x-[1px] -translate-y-0.5 text-3xl font-bold" href=https://wangmerlyn.github.io/></a>
<a class="btn-dark ml-6 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]"></a></div><a class="btn-menu relative z-50 -mr-8 flex h-[5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"></a>
<script>const metaTheme=document.querySelector('meta[name="theme-color"]'),htmlClass=document.documentElement.classList;setTimeout(()=>htmlClass.remove("not-ready"),10);const setDark=e=>{metaTheme.setAttribute("content",e?"#000":"#fbfbfb"),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)"),darkVal=localStorage.getItem("dark");setDark(darkVal?darkVal==="true":darkScheme.matches),darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")});const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="mt-12 flex justify-center space-x-10 dark:invert lg:mt-0 lg:ml-12 lg:items-center lg:space-x-6"><a class="h-8 w-8 [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./github.svg) href=https://github.com/Wangmerlyn target=_blank></a>
<a class="h-8 w-8 [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./rss.svg) href=https://wangmerlyn.github.io/index.xml target=_blank></a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100%-10rem)] max-w-3xl px-8 pt-20 pb-32 dark:prose-invert"><article><header class=mb-20><h1 class="!my-0 pb-2.5">APF review</h1><div class="text-sm opacity-60"><time>Jul 26, 2022</time>
<span class=mx-1>&#183;</span>
<span>Wangmerlyn</span></div></header><section><h1 id=communication-efficient-federated-learning-with-adaptive-parameter-freezing-review>&ldquo;Communication-Efficient Federated Learning with Adaptive Parameter Freezing&rdquo; review</h1><h2 id=back-ground>Back Ground</h2><p>There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.</p><h3 id=way-to-reduce-the-communication-amount-for-distributed-model-training>Way to reduce the communication amount for distributed model training</h3><ul><li><em>quantizing</em> the update into fewer bits</li><li><em>sparsify</em> local updates</li></ul><h3 id=the-challenges-of-boosting-federal-learning-efficiency-by-skipping-out-of-stablized-parameters>The Challenges of boosting federal learning efficiency by skipping out of stablized parameters</h3><ul><li>How to identify stabilized parameters effectively</li><li>How to ensure that the model will converge to the optimal state</li></ul><h2 id=the-parameter-changing-feature>The parameter changing feature</h2><p>By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{*} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^* $.</p><h2 id=identify-stablized-parameters>Identify stablized parameters</h2><p>At first the authors proposed a indicator called <em>effective perbutation</em> $$P_K^r=\frac{||\sum_{i=0}^{r} \mu_{k-i}||}{\sum_{i=0}^{r}||\mu_{k-i}||}$$ to describe the stableness of a certain parameter. Later the indicator was improved using exponential average $P_K=\frac{E_K}{E_K^{abs}}$ where $E_K = \alpha E_{K-1}+(1-\alpha)\Delta_K$, $E_{K}^{abs}=\alpha E_{K-1}+(1-\alpha)|\Delta_K|$ to avoid wasting storage for model snapshots and adjust the focus on recent model updates. If the effective perbutation hits below a threshold, the parameter is considered stable. The threshold can be adaptively changing.</p><h2 id=adaptive-parameter-freezing>Adaptive parameter freezing</h2><h3 id=partial-syncing>Partial syncing</h3><p>Freezing parameters locally on individual worker has been proven to be working poorly since works deal with non IDD data in a federated learning task, causing parameters to diverge after previous stablized parameters are out of sync.</p><h3 id=permanent-freezing>Permanent freezing</h3><p>Permanent parameter freezing is gonna result in the loss of acurracy since parameters that are freezed when they are stable halfway in the training are still probably not optimal.</p><h3 id=parameter-freezing-principles>Parameter freezing principles</h3><p>From the 2 observations above, the authors drew the following principles for adaptive parameter freezing.</p><ol><li>Stablized parameters must be kept unchanged on each worker.</li><li>Temporary stableness must be handled.</li></ol><h3 id=apf-method>APF method</h3><p>With the 2 principles above combined, the authors raised an &ldquo;adaptive parameter freezing&rdquo; mechanism. While keeping an eye on those stablized parameters, the freezing period should also be adaptively changing. After a parameter is recognized as stabilized, it should freezed for a specific amount of time that is controlled by a adaptive mechanism which is similar to TCP AIMD. By checking whether the parameter remains stable after unfreezing, the mechanism changes the freezing period adaptively. Taking efficiency into consideration, effective perbutation check is not necessary to be conducted once after each iteration,</p><h3 id=agressive-apf>Agressive APF</h3><p>While dealing with over-parameterized models, certain parameters may not tend to converge due to their non-convex nature. Agressive APF works like APF but also randomly freeze parameters therefore filter out the training of less needed parameters.</p></section><footer class="mt-12 flex flex-wrap"><a class="mr-1.5 mb-1.5 rounded-lg bg-black/[3%] px-5 py-2 no-underline dark:bg-white/[8%]" href=https://wangmerlyn.github.io/tags/distributed-learning>distributed learning</a>
<a class="mr-1.5 mb-1.5 rounded-lg bg-black/[3%] px-5 py-2 no-underline dark:bg-white/[8%]" href=https://wangmerlyn.github.io/tags/ai-system>AI system</a></footer><nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]"><a class="flex w-1/2 items-center p-6 pr-3 no-underline" href=https://wangmerlyn.github.io/posts/good-comment/><span class=mr-1.5>←</span><span>Good Comment</span></a></nav><div id=disqus_thread></div><script>const disqusShortname="merlynwang",script=document.createElement("script");script.src="https://"+disqusShortname+".disqus.com/embed.js",script.setAttribute("data-timestamp",+new Date),document.head.appendChild(script)</script></article></main><footer class="opaco mx-auto flex h-[5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"><div class=pb-2><script src=https://utteranc.es/client.js repo=wangmerlyn/wangmerlyn.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div><div class=mr-auto>&copy; 2022
<a class=link href=https://wangmerlyn.github.io/></a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>Powered by Hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>▷ Paper 6</a></footer></body></html>