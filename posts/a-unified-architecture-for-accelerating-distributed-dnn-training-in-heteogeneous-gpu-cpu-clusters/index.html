<!doctype html><html class="not-ready text-sm lg:text-base" lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>A Unified Architecture for Accelerating Distributed DNN Training in Heteogeneous GPU/CPU Clusters -</title><meta name=theme-color><meta name=description content="Problems Sub-optimal inter-machine communication Sub-optimal intra-machine communication The CPU bottleneck Proposed Solutions An optimal inter-machine communication strategy that is generic and unfies all-reduce and PS Intra-machine optimization that accelerates communication inside GPU machines with diverse topology Summation Service that aggregates gradients oon CPUs and moves parameter updates to GPUs Design and Implementation Opportunity Findings from ByteDance Clusters:
Avg. CPU utilization: 20% ~ 35% 20% ~ 45% run non-dist jobs: Bandwidth unused 在异质的集群中存在空闲的CPU使用和带宽剩余，目标在于平衡空闲的资源"><meta name=author content="wang merlyn"><link rel="preload stylesheet" as=style href=https://Wangmerlyn.github.io/main.min.css><link rel=preload as=image href=https://Wangmerlyn.github.io/theme.%7B%7B%20if%20site.Params.monoDarkIcon%20%7D%7Dsvg%7B%7B%20else%20%7D%7Dpng%7B%7B%20end%20%7D%7D><link rel=preload as=image href="https://www.gravatar.com/avatar/4cb92498631c99b1dca3aaf0b8488462?s=160&d=identicon"><link rel=preload as=image href=https://Wangmerlyn.github.io/github.svg><link rel=preload as=image href=https://Wangmerlyn.github.io/rss.svg><link rel=icon href=https://Wangmerlyn.github.io/favicon.ico><link rel=apple-touch-icon href=https://Wangmerlyn.github.io/apple-touch-icon.png><meta name=generator content="Hugo 0.105.0"><meta property="og:title" content="A Unified Architecture for Accelerating Distributed DNN Training in Heteogeneous GPU/CPU Clusters"><meta property="og:description" content="OSDI'20"><meta property="og:type" content="article"><meta property="og:url" content="https://Wangmerlyn.github.io/posts/a-unified-architecture-for-accelerating-distributed-dnn-training-in-heteogeneous-gpu-cpu-clusters/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-30T16:18:08+08:00"><meta property="article:modified_time" content="2022-07-30T16:18:08+08:00"><meta itemprop=name content="A Unified Architecture for Accelerating Distributed DNN Training in Heteogeneous GPU/CPU Clusters"><meta itemprop=description content="OSDI'20"><meta itemprop=datePublished content="2022-07-30T16:18:08+08:00"><meta itemprop=dateModified content="2022-07-30T16:18:08+08:00"><meta itemprop=wordCount content="145"><meta itemprop=keywords content="distributed learning,AI-systems,"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Unified Architecture for Accelerating Distributed DNN Training in Heteogeneous GPU/CPU Clusters"><meta name=twitter:description content="OSDI'20"></head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[5rem] max-w-3xl px-8 lg:justify-center"><div class="relative z-50 mr-auto flex items-center"><a class="-translate-x-[1px] -translate-y-0.5 text-3xl font-bold" href=https://Wangmerlyn.github.io></a>
<a class="btn-dark ml-6 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]"></a></div><a class="btn-menu relative z-50 -mr-8 flex h-[5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"></a>
<script>const metaTheme=document.querySelector('meta[name="theme-color"]'),htmlClass=document.documentElement.classList;setTimeout(()=>htmlClass.remove("not-ready"),10);const setDark=e=>{metaTheme.setAttribute("content",e?"#000":"#fbfbfb"),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)"),darkVal=localStorage.getItem("dark");setDark(darkVal?darkVal==="true":darkScheme.matches),darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")});const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="mt-12 flex justify-center space-x-10 dark:invert lg:mt-0 lg:ml-12 lg:items-center lg:space-x-6"><a class="h-8 w-8 [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./github.svg) href=https://github.com/Wangmerlyn target=_blank></a>
<a class="h-8 w-8 [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./rss.svg) href=https://Wangmerlyn.github.io/index.xml target=_blank></a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100%-10rem)] max-w-3xl px-8 pt-20 pb-32 dark:prose-invert"><article><header class=mb-20><h1 class="!my-0 pb-2.5">A Unified Architecture for Accelerating Distributed DNN Training in Heteogeneous GPU/CPU Clusters</h1><div class="text-sm opacity-60"><time>Jul 30, 2022</time>
<span class=mx-1>&#183;</span>
<span>wang merlyn</span></div></header><section><h2 id=problems>Problems</h2><ul><li>Sub-optimal inter-machine communication</li><li>Sub-optimal intra-machine communication</li><li>The CPU bottleneck</li></ul><h2 id=proposed-solutions>Proposed Solutions</h2><ul><li>An optimal inter-machine communication strategy that is generic and unfies all-reduce and PS</li><li>Intra-machine optimization that accelerates communication inside GPU machines with diverse topology</li><li>Summation Service that aggregates gradients oon CPUs and moves parameter updates to GPUs</li></ul><h1 id=design-and-implementation>Design and Implementation</h1><h1 id=opportunity>Opportunity</h1><p>Findings from ByteDance Clusters:</p><ul><li>Avg. CPU utilization: 20% ~ 35%</li><li>20% ~ 45% run non-dist jobs: Bandwidth unused</li></ul><p>在异质的集群中存在空闲的CPU使用和带宽剩余，目标在于平衡空闲的资源</p><h2 id=inter-machine-communication>Inter-machine Communication</h2><p>PS只使用GPU到CPU间的网络，也就是说当CPU设备不够多的时候，GPU的带宽并不会被充分的使用（例如有2个CPU，3个GPU，那么相比CPU设备的带宽负载，GPU设备的带宽负载只有2/3）
all-reduce只使用GPU间的带宽，没有利用CPU带宽
现在问题在于，如果要结合PS和all-reduce方法，怎么平衡网络负载</p><h3 id=best-partition-strategy>Best Partition Strategy</h3><p>对于一个CPU机器，SS的负载决定了网络的交通情况，如果1个SS负责$x%$的参数，那么每次训练的时候CPU机器都会向每个GPU发送$x%M$的数据。
然而我们可以发现，每个GPU的工作交通是由SS和CS共同决定的，这里进行分类$SS_{CPU}$和$SS_{GPU}$。
在网络中，CS应该总共发送M的数据，当CS和位于同一台GPU上的SS进行通信的时候，是不会占用网络带宽的，这时候CS只使用$M-M_{SS_{GPU}}$的网络带宽
与此同时，1个GPU机器上的SS要求接受和发送来自其它(n-1)台机器的数据$(n-1)M_{SS_{GPU}}$，也就是说，一台GPU机器在给定带宽为$B$的时候，通信时间就是
$t_g=\frac{M+(n-2)M_{SS_{GPU}}}{B}$
相应的，当通信网络中有CPU存在的时候，每个CPU的通信时间就会变成
$t_c=\frac{M_{SS_{CPU}}}{B}$
由于模型通信总量限制
$M=kM_{SS_{CPU}}+nM_{SS_{GPU}}$
由于通信瓶颈，我们为了优化网络性能，目标就是在于优化函数$\min \max(t_c.t_g)$，进行规划之后可以解得
$t_{opt}=\frac{2n(n-1)M}{(n^2+kn-2k)B}$</p><h2 id=intra-machine-communication>Intra-machine Communication</h2><p>瓶颈发生在CPU和PCIe switch之间，现在问题的解决方法在于减少在这条连接上的通信负载，这其中会涉及到Ring All-Reduce通信原理和负载计算，建议参照<a href=https://freewsy.ml/posts/nccl-allreduce/>博客</a></p><h3 id=solution-cpu-assisted-aggregation>Solution: CPU-assisted Aggregation</h3><ol><li>每个CPU管理的集群中进行reduce-scatter</li><li>每个CPU集群下的GPU将各自持有的拥有全体整合数据的梯度片段通过总线进行复制给CPU</li><li>CPU之间进行相加整合</li></ol><p>这样就可以让瓶颈链路上的数据传输总量降低为1次模型数据传输量</p><ul><li>CPU-assisted aggregation outperforms MPI/NCCL by 24% in theory</li><li><strong>Principle</strong>: avoid direct copy between GPUs under different PCIe switches</li></ul><h2 id=cpu-bottleneck>CPU bottleneck</h2><p>We need a module that can run on CPUs with high performance
通过观察可以发现，整个在CPU PS上进行的操作其实就是优化函数$W&rsquo;=W-f(\nabla W)$，我们可以将整个过程分为两个部分</p><ul><li>梯度相加（CPU friendly）</li><li>参数更新（heavy for CPU）</li></ul><p>解决方法：让GPU负责参数更新</p></section><footer class="mt-12 flex flex-wrap"><a class="mr-1.5 mb-1.5 rounded-lg bg-black/[3%] px-5 py-2 no-underline dark:bg-white/[8%]" href=https://Wangmerlyn.github.io/tags/distributed-learning>distributed learning</a>
<a class="mr-1.5 mb-1.5 rounded-lg bg-black/[3%] px-5 py-2 no-underline dark:bg-white/[8%]" href=https://Wangmerlyn.github.io/tags/ai-systems>AI-systems</a></footer><nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]"><a class="flex w-1/2 items-center p-6 pr-3 no-underline" href=https://Wangmerlyn.github.io/posts/consistent-hashing/><span class=mr-1.5>←</span><span>Consistent Hashing</span></a>
<a class="ml-auto flex w-1/2 items-center justify-end p-6 pl-3 no-underline" href=https://Wangmerlyn.github.io/posts/unified-all-reduce-and-ps/><span>Unified All Reduce and PS</span><span class=ml-1.5>→</span></a></nav></article></main><div class=pb-2><script src=https://utteranc.es/client.js repo=wangmerlyn/wangmerlyn.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div><footer class="opaco mx-auto flex h-[5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"><div class=mr-auto>&copy; 2022
<a class=link href=https://Wangmerlyn.github.io></a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>Powered by Hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>▷ Paper 6</a></footer></body></html>