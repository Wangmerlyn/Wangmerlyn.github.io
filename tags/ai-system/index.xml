<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI system on</title><link>freewsy.ml/tags/ai-system/</link><description>Recent content in AI system on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 26 Jul 2022 16:09:39 +0800</lastBuildDate><atom:link href="freewsy.ml/tags/ai-system/index.xml" rel="self" type="application/rss+xml"/><item><title>APF review</title><link>freewsy.ml/posts/my-first/</link><pubDate>Tue, 26 Jul 2022 16:09:39 +0800</pubDate><guid>freewsy.ml/posts/my-first/</guid><description>&amp;ldquo;Communication-Efficient Federated Learning with Adaptive Parameter Freezing&amp;rdquo; review Back Ground There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.
Way to reduce the communication amount for distributed model training quantizing the update into fewer bits sparsify local updates The Challenges of boosting federal learning efficiency by skipping out of stablized parameters How to identify stabilized parameters effectively How to ensure that the model will converge to the optimal state The parameter changing feature By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{*} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^* $.</description><content>&lt;h1 id="communication-efficient-federated-learning-with-adaptive-parameter-freezing-review">&amp;ldquo;Communication-Efficient Federated Learning with Adaptive Parameter Freezing&amp;rdquo; review&lt;/h1>
&lt;h2 id="back-ground">Back Ground&lt;/h2>
&lt;p>There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.&lt;/p>
&lt;h3 id="way-to-reduce-the-communication-amount-for-distributed-model-training">Way to reduce the communication amount for distributed model training&lt;/h3>
&lt;ul>
&lt;li>&lt;em>quantizing&lt;/em> the update into fewer bits&lt;/li>
&lt;li>&lt;em>sparsify&lt;/em> local updates&lt;/li>
&lt;/ul>
&lt;h3 id="the-challenges-of-boosting-federal-learning-efficiency-by-skipping-out-of-stablized-parameters">The Challenges of boosting federal learning efficiency by skipping out of stablized parameters&lt;/h3>
&lt;ul>
&lt;li>How to identify stabilized parameters effectively&lt;/li>
&lt;li>How to ensure that the model will converge to the optimal state&lt;/li>
&lt;/ul>
&lt;h2 id="the-parameter-changing-feature">The parameter changing feature&lt;/h2>
&lt;p>By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{*} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^* $.&lt;/p>
&lt;h2 id="identify-stablized-parameters">Identify stablized parameters&lt;/h2>
&lt;p>At first the authors proposed a indicator called &lt;em>effective perbutation&lt;/em> $$P_K^r=\frac{||\sum_{i=0}^{r} \mu_{k-i}||}{\sum_{i=0}^{r}||\mu_{k-i}||}$$ to describe the stableness of a certain parameter. Later the indicator was improved using exponential average $P_K=\frac{E_K}{E_K^{abs}}$ where $E_K = \alpha E_{K-1}+(1-\alpha)\Delta_K$, $E_{K}^{abs}=\alpha E_{K-1}+(1-\alpha)|\Delta_K|$ to avoid wasting storage for model snapshots and adjust the focus on recent model updates. If the effective perbutation hits below a threshold, the parameter is considered stable. The threshold can be adaptively changing.&lt;/p>
&lt;h2 id="adaptive-parameter-freezing">Adaptive parameter freezing&lt;/h2>
&lt;h3 id="partial-syncing">Partial syncing&lt;/h3>
&lt;p>Freezing parameters locally on individual worker has been proven to be working poorly since works deal with non IDD data in a federated learning task, causing parameters to diverge after previous stablized parameters are out of sync.&lt;/p>
&lt;h3 id="permanent-freezing">Permanent freezing&lt;/h3>
&lt;p>Permanent parameter freezing is gonna result in the loss of acurracy since parameters that are freezed when they are stable halfway in the training are still probably not optimal.&lt;/p>
&lt;h3 id="parameter-freezing-principles">Parameter freezing principles&lt;/h3>
&lt;p>From the 2 observations above, the authors drew the following principles for adaptive parameter freezing.&lt;/p>
&lt;ol>
&lt;li>Stablized parameters must be kept unchanged on each worker.&lt;/li>
&lt;li>Temporary stableness must be handled.&lt;/li>
&lt;/ol>
&lt;h3 id="apf-method">APF method&lt;/h3>
&lt;p>With the 2 principles above combined, the authors raised an &amp;ldquo;adaptive parameter freezing&amp;rdquo; mechanism. While keeping an eye on those stablized parameters, the freezing period should also be adaptively changing. After a parameter is recognized as stabilized, it should freezed for a specific amount of time that is controlled by a adaptive mechanism which is similar to TCP AIMD. By checking whether the parameter remains stable after unfreezing, the mechanism changes the freezing period adaptively. Taking efficiency into consideration, effective perbutation check is not necessary to be conducted once after each iteration,&lt;/p>
&lt;h3 id="agressive-apf">Agressive APF&lt;/h3>
&lt;p>While dealing with over-parameterized models, certain parameters may not tend to converge due to their non-convex nature. Agressive APF works like APF but also randomly freeze parameters therefore filter out the training of less needed parameters.&lt;/p></content></item></channel></rss>