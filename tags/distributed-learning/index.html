<!doctype html><html lang=en><head><title>distributed learning ::</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=freewsy.ml/tags/distributed-learning/><link rel=stylesheet href=freewsy.ml/assets/style.css><link rel=stylesheet href=assets/%25!s%28%3cnil%3e%29.css><link rel=apple-touch-icon href=freewsy.ml/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=freewsy.ml/img/favicon/orange.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta property="og:title" content="distributed learning"><meta property="og:description" content><meta property="og:url" content="freewsy.ml/tags/distributed-learning/"><meta property="og:site_name" content><meta property="og:image" content="img/favicon/%!s().png"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><link href=freewsy.ml/tags/distributed-learning/index.xml rel=alternate type=application/rss+xml title></head><body><div class="container headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=freewsy.ml><div class=logo>Terminal</div></a></div></div></header><div class=content><div class=posts><div class="post on-list"><h1 class=post-title><a href=freewsy.ml/posts/nccl-allreduce/>Nccl Allreduce</a></h1><div class=post-meta><span class=post-date>2022-07-28</span>
<span class=post-author>:: wang merlyn</span></div><span class=post-tags>#<a href=freewsy.ml/tags/distributed-learning/>distributed learning</a>&nbsp;
#<a href=freewsy.ml/tags/ai-systems/>AI-systems</a>&nbsp;
#<a href=freewsy.ml/tags/all-reduce/>all-reduce</a>&nbsp;
#<a href=freewsy.ml/tags/nccl/>nccl</a>&nbsp;
#<a href=freewsy.ml/tags/network/>Network</a>&nbsp;
#<a href=freewsy.ml/tags/topology/>Topology</a>&nbsp;</span><div class=post-content>what the hell algorithmn is nccl using</div><div><a class="read-more button" href=freewsy.ml/posts/nccl-allreduce/>→</a></div></div><div class="post on-list"><h1 class=post-title><a href=freewsy.ml/posts/my-first/>APF review</a></h1><div class=post-meta><span class=post-date>2022-07-26</span>
<span class=post-author>:: Wangmerlyn</span></div><span class=post-tags>#<a href=freewsy.ml/tags/distributed-learning/>distributed learning</a>&nbsp;
#<a href=freewsy.ml/tags/ai-system/>AI system</a>&nbsp;</span><div class=post-content>“Communication-Efficient Federated Learning with Adaptive Parameter Freezing” review Back Ground There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.
Way to reduce the communication amount for distributed model training quantizing the update into fewer bits sparsify local updates The Challenges of boosting federal learning efficiency by skipping out of stablized parameters How to identify stabilized parameters effectively How to ensure that the model will converge to the optimal state The parameter changing feature By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{<em>} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^</em> $.</div><div><a class="read-more button" href=freewsy.ml/posts/my-first/>→</a></div></div><div class=pagination><div class=pagination__buttons></div></div></div></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2022 Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=freewsy.ml/assets/main.js></script>
<script src=freewsy.ml/assets/prism.js></script></div></body></html>